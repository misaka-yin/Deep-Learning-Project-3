{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939f8bcf",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "All the libraries and utilities we use across Tasks 1–5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66984168",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ─── Numerical computing ────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ─── PyTorch core ───────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# ─── Standard library ───────────────────────────────────────────────────────────\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ─── Numerical computing ────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "\n",
    "# ─── PyTorch core ───────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ─── TorchVision ───────────────────────────────────────────────────────────────\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# ─── Data loading ──────────────────────────────────────────────────────────────\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ─── Image handling ───────────────────────────────────────────────────────────\n",
    "from PIL import Image\n",
    "\n",
    "# ─── Visualization ─────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Progress bar ──────────────────────────────────────────────────────────────\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── Custom utilities ──────────────────────────────────────────────────────────\n",
    "from utils import (\n",
    "    fgsm_attack,\n",
    "    pgd_attack,\n",
    "    pgd_patch_attack,\n",
    "    AdvDataset,\n",
    "    compute_topk_accuracy,\n",
    "    RAW_TRANSFORM,\n",
    "    NORM_TRANSFORM,\n",
    "    numeric_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534dc35",
   "metadata": {},
   "source": [
    "## Task 1: Baseline Evaluation\n",
    "\n",
    "In this task we load a pre-trained ResNet-34, prepare the ImageNet subset, build the label mapping, and compute top-1 and top-5 accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbafcd",
   "metadata": {},
   "source": [
    "1. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd718ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet-34 with ImageNet-1K V1 weights and set to evaluation mode\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd0c39",
   "metadata": {},
   "source": [
    "2. Define Transforms & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138329c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters\n",
    "mean_norms = np.array([0.485, 0.456, 0.406])\n",
    "std_norms  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Compose transforms: to tensor + normalize\n",
    "plain_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_norms, std=std_norms)\n",
    "])\n",
    "\n",
    "# Path to the 100-class test subset\n",
    "dataset_path = \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/TestDataSet\"\n",
    "\n",
    "# Load the images from folder structure\n",
    "dataset = datasets.ImageFolder(root=dataset_path,\n",
    "                               transform=plain_transforms)\n",
    "\n",
    "# Quick sanity check\n",
    "print(dataset.classes[:20])   # first 20 synsets\n",
    "print(\"Total\", len(dataset.classes), \"classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112e485",
   "metadata": {},
   "source": [
    "3. Read labels_list.json & Build Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737372e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get folder synsets\n",
    "synsets = dataset.classes\n",
    "\n",
    "# Load the JSON list: [\"0: accordion\", \"1: acoustic guitar\", ...]\n",
    "with open(os.path.join(dataset_path, \"labels_list.json\"), \"r\") as f:\n",
    "    label_list = json.load(f)\n",
    "\n",
    "# Extract only class names\n",
    "json_classnames = [x.split(\": \")[1] for x in label_list]\n",
    "\n",
    "print(\"First 5 folder synsets:\", synsets[:5])\n",
    "print(\"First 5 JSON class names:\", json_classnames[:5])\n",
    "\n",
    "# Build reverse map: name → string index\n",
    "r_label_list = {name: idx_str for (idx_str, name) in\n",
    "                (e.split(\": \",1) for e in label_list)}\n",
    "\n",
    "# Construct numeric_labels: global ImageNet indices for each folder class\n",
    "numeric_labels = []\n",
    "for syn, name in zip(synsets, json_classnames):\n",
    "    numeric_labels.append(int(r_label_list[name]))\n",
    "\n",
    "print(\"First 5 numeric labels:\", numeric_labels[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efad366",
   "metadata": {},
   "source": [
    "4. Compute Baseline Top-1 / Top-5 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87932a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to compute top-k accuracy\n",
    "def compute_topk_accuracy(outputs, targets, topk=(1,5)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = outputs.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = {}\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res[k] = (correct_k.item() * 100.0) / batch_size\n",
    "    return res\n",
    "\n",
    "# Create DataLoader\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Accumulate accuracy\n",
    "sum1 = sum5 = n = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, folder_idxs in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        # Map folder index (0–99) to global 0–999 labels\n",
    "        targets = torch.tensor(\n",
    "            [numeric_labels[i] for i in folder_idxs.tolist()],\n",
    "            dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        acc = compute_topk_accuracy(outputs, targets, topk=(1,5))\n",
    "        bsz = targets.size(0)\n",
    "        sum1 += acc[1] * bsz / 100.0\n",
    "        sum5 += acc[5] * bsz / 100.0\n",
    "        n   += bsz\n",
    "\n",
    "# Final metrics\n",
    "baseline_top1 = 100.0 * sum1 / n\n",
    "baseline_top5 = 100.0 * sum5 / n\n",
    "print(f\"Baseline Top-1: {baseline_top1:.2f}%\")\n",
    "print(f\"Baseline Top-5: {baseline_top5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222a9c1",
   "metadata": {},
   "source": [
    "## Task 2: Pixel-wise Attack (FGSM)\n",
    "In this task we prepare the data, build the label mapping, and define all the attack and utility functions we’ll need for FGSM (and later PGD) attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ffed7",
   "metadata": {},
   "source": [
    "### 1. Preprocessing Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6705206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize→crop→to_tensor (for adversarial generation)\n",
    "RAW_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# to_tensor→normalize (for evaluation)\n",
    "NORM_TRANSFORM = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25705362",
   "metadata": {},
   "source": [
    "2. Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ac930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load the dataset folder structure\n",
    "_ds = datasets.ImageFolder(dataset_path, transform=RAW_TRANSFORM)\n",
    "\n",
    "# 2) read the \"idx: class\" list\n",
    "_entries = json.load(open(os.path.join(dataset_path, \"labels_list.json\")))\n",
    "\n",
    "# 3) split into (synset, idx_str)\n",
    "_pairs = [e.split(\": \", 1) for e in _entries]\n",
    "\n",
    "# 4) build synset → global ImageNet idx map\n",
    "_syn2idx = {syn: int(idx) for idx, syn in _pairs}\n",
    "\n",
    "# 5) create a 500-long list of global labels\n",
    "numeric_labels = [_syn2idx[s] for s in _ds.classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef66202",
   "metadata": {},
   "source": [
    "3. Utility & Attack Functions\n",
    "3.1 Top-k Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_accuracy(outputs: torch.Tensor,\n",
    "                          targets: torch.Tensor,\n",
    "                          topk=(1,5)) -> dict:\n",
    "    \"\"\"Return {k: accuracy%} for each k in topk.\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = outputs.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = {}\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res[k] = (correct_k.item() * 100.0) / batch_size\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ec57c",
   "metadata": {},
   "source": [
    "3.2 FGSM Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, x: torch.Tensor, y: torch.Tensor,\n",
    "                epsilon: float, mean: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Single-step FGSM: x in [0,1], y is global ImageNet label.\n",
    "    \"\"\"\n",
    "    x_adv = x.clone().detach().to(mean.device)\n",
    "    x_adv.requires_grad_()\n",
    "    logits = model((x_adv - mean)/std)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    return torch.clamp(x_adv + epsilon * x_adv.grad.sign(), 0.0, 1.0).detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cce993",
   "metadata": {},
   "source": [
    "3.3 Adversarial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvDataset(Dataset):\n",
    "    \"\"\"Load adversarial PNGs + their labels from a folder.\"\"\"\n",
    "    def __init__(self, image_dir, labels, transform):\n",
    "        self.files = sorted(os.listdir(image_dir))\n",
    "        self.dir   = image_dir\n",
    "        self.labels= labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = os.path.join(self.dir, self.files[i])\n",
    "        img  = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(img), self.labels[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee4b0a",
   "metadata": {},
   "source": [
    "3.4 PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, x: torch.Tensor, y: torch.Tensor,\n",
    "               epsilon: float, alpha: float, iters: int,\n",
    "               mean: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Multi-step PGD attack on full image.\"\"\"\n",
    "    x_adv, x_orig = x.clone().detach(), x.clone().detach()\n",
    "    for _ in range(iters):\n",
    "        x_adv.requires_grad_()\n",
    "        logits = model((x_adv - mean)/std)\n",
    "        F.cross_entropy(logits, y).backward()\n",
    "        x_adv = x_adv + alpha * x_adv.grad.sign()\n",
    "        x_adv = torch.max(torch.min(x_adv, x_orig + epsilon),\n",
    "                          x_orig - epsilon)\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0).detach()\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670d245",
   "metadata": {},
   "source": [
    "3.5 PGD Patch Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_patch_attack(model, x: torch.Tensor, y: torch.Tensor,\n",
    "                     epsilon: float, alpha: float, iters: int,\n",
    "                     mean: torch.Tensor, std: torch.Tensor,\n",
    "                     patch_size: int,\n",
    "                     random_patch: bool=False) -> torch.Tensor:\n",
    "    \"\"\"PGD attack restricted to a single patch of size patch_size.\"\"\"\n",
    "    x_adv, x_orig = x.clone().detach(), x.clone().detach()\n",
    "    B, C, H, W = x.shape\n",
    "    if random_patch:\n",
    "        top  = torch.randint(0, H-patch_size+1, (1,)).item()\n",
    "        left = torch.randint(0, W-patch_size+1, (1,)).item()\n",
    "    else:\n",
    "        top  = (H-patch_size)//2\n",
    "        left = (W-patch_size)//2\n",
    "    mask = torch.zeros_like(x_adv)\n",
    "    mask[:, :, top:top+patch_size, left:left+patch_size] = 1\n",
    "\n",
    "    for _ in range(iters):\n",
    "        x_adv.requires_grad_()\n",
    "        logits = model((x_adv - mean)/std)\n",
    "        F.cross_entropy(logits, y).backward()\n",
    "        grad = x_adv.grad.sign()\n",
    "        x_adv = x_adv + alpha * grad * mask\n",
    "        x_adv = torch.max(torch.min(x_adv, x_orig + epsilon*mask),\n",
    "                          x_orig - epsilon*mask)\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0).detach()\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca8ed4",
   "metadata": {},
   "source": [
    "### 4. Generate Adversarial Examples with FGSM\n",
    "In this step we iterate over the clean images, apply the FGSM attack to each one, save the perturbed images, and collect a few examples where the model flips its prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epsilon    = 0.02\n",
    "dataset_pa = dataset_path\n",
    "adv_dir    = \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/Adversarial_Test_Set_1\"\n",
    "os.makedirs(adv_dir, exist_ok=True)\n",
    "\n",
    "# DataLoader for raw images\n",
    "raw_ds = datasets.ImageFolder(dataset_pa, transform=RAW_TRANSFORM)\n",
    "loader = DataLoader(raw_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# Normalization tensors for the attack\n",
    "mean = torch.tensor([0.485,0.456,0.406], device=device).view(1,3,1,1)\n",
    "std  = torch.tensor([0.229,0.224,0.225], device=device).view(1,3,1,1)\n",
    "\n",
    "# Load model\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "max_linf    = 0.0\n",
    "mis_examples = []\n",
    "\n",
    "for idx, (img, folder_idx) in enumerate(tqdm(loader)):\n",
    "    img      = img.to(device)\n",
    "    true_idx = numeric_labels[folder_idx.item()]\n",
    "    y        = torch.tensor([true_idx], device=device)\n",
    "\n",
    "    # Perform FGSM attack\n",
    "    adv = fgsm_attack(model, img, y, epsilon, mean, std)\n",
    "\n",
    "    # Track L∞ distance\n",
    "    linf = (adv - img).abs().max().item()\n",
    "    max_linf = max(max_linf, linf)\n",
    "    assert linf <= epsilon + 1e-6, f\"L∞={linf:.5f} > ε\"\n",
    "\n",
    "    # Save adversarial image\n",
    "    save_path = os.path.join(adv_dir, f\"{idx:04d}.png\")\n",
    "    save_image(adv.cpu(), save_path)\n",
    "\n",
    "    # Collect first 5 “correct→incorrect” examples\n",
    "    if len(mis_examples) < 5:\n",
    "        with torch.no_grad():\n",
    "            pred_o = model((img - mean)/std).argmax(1).item()\n",
    "            pred_a = model((adv - mean)/std).argmax(1).item()\n",
    "        print(f\"true {true_idx}, before {pred_o}, after {pred_a}\")\n",
    "        if pred_o == true_idx and pred_a != true_idx:\n",
    "            mis_examples.append((img.cpu(), adv.cpu(), true_idx))\n",
    "\n",
    "print(f\"Maximum L∞ distance over all adversarial samples: {max_linf:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481a99a",
   "metadata": {},
   "source": [
    "### 5. Evaluate FGSM Attack Performance\n",
    "Now we load the saved adversarial images, build a DataLoader, and compute top-1 and top-5 accuracy drops relative to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build labels list: 5 images per class × 100 classes = 500 labels\n",
    "expanded_labels = [numeric_labels[i] for i in range(100) for _ in range(5)]\n",
    "\n",
    "# Dataset & DataLoader for adversarial images\n",
    "adv_ds     = AdvDataset(adv_dir, expanded_labels, NORM_TRANSFORM)\n",
    "adv_loader = DataLoader(adv_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "sum1 = sum5 = n = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in adv_loader:\n",
    "        x, y = x.to(device), torch.tensor(y, device=device)\n",
    "        logits = model(x)  # already normalized inside fgsm_attack\n",
    "        acc = compute_topk_accuracy(logits, y, topk=(1,5))\n",
    "        bsz = y.size(0)\n",
    "        sum1 += acc[1] * bsz / 100.0\n",
    "        sum5 += acc[5] * bsz / 100.0\n",
    "        n    += bsz\n",
    "\n",
    "fgsm_top1 = 100.0 * sum1 / n\n",
    "fgsm_top5 = 100.0 * sum5 / n\n",
    "\n",
    "# Compute drop relative to baseline_top1 and baseline_top5\n",
    "drop1 = (baseline_top1 - fgsm_top1) / baseline_top1 * 100\n",
    "drop5 = (baseline_top5 - fgsm_top5) / baseline_top5 * 100\n",
    "\n",
    "print(f\"Baseline Top-1: {baseline_top1:.2f}%\")\n",
    "print(f\"Baseline Top-5: {baseline_top5:.2f}%\")\n",
    "print(f\"FGSM Top-1:     {fgsm_top1:.2f}%  (Drop {drop1:.2f}%)\")\n",
    "print(f\"FGSM Top-5:     {fgsm_top5:.2f}%  (Drop {drop5:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b8809",
   "metadata": {},
   "source": [
    "## Task 3: Multi-step PGD Attack\n",
    "\n",
    "In this task we generate a stronger adversarial set using the Projected Gradient Descent (PGD) method, then evaluate its effect on ResNet-34, and finally visualize a few “correct→incorrect” examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf53c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Setup PGD Parameters & Paths\n",
    "\n",
    "\n",
    "# Directory for saving PGD adversarial images\n",
    "pgd_dir = \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/Adversarial_Test_Set_2\"\n",
    "os.makedirs(pgd_dir, exist_ok=True)\n",
    "\n",
    "# Attack hyperparameters\n",
    "epsilon = 0.02\n",
    "alpha   = 0.005\n",
    "iters   = 10\n",
    "\n",
    "# DataLoader for raw images using RAW_TRANSFORM\n",
    "raw_ds = datasets.ImageFolder(dataset_pa, transform=RAW_TRANSFORM)\n",
    "loader = DataLoader(raw_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# Containers for tracking examples\n",
    "mis_examples_pgd = []\n",
    "max_linf_pgd     = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965cfd3",
   "metadata": {},
   "source": [
    "### 2. Generate PGD Adversarial Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (img, folder_idx) in enumerate(tqdm(loader)):\n",
    "    img      = img.to(device)\n",
    "    y        = torch.tensor([numeric_labels[folder_idx.item()]], device=device)\n",
    "    true_idx = y.item()\n",
    "\n",
    "    # Run PGD attack\n",
    "    adv = pgd_attack(model, img, y, epsilon, alpha, iters, mean, std)\n",
    "\n",
    "    # Track max L-infinity perturbation\n",
    "    linf = (adv - img).abs().max().item()\n",
    "    max_linf_pgd = max(max_linf_pgd, linf)\n",
    "    assert linf <= epsilon + 1e-6, f\"PGD L∞={linf:.5f} > ε\"\n",
    "\n",
    "    # Save adversarial image\n",
    "    save_path = os.path.join(pgd_dir, f\"{idx:04d}.png\")\n",
    "    save_image(adv.cpu(), save_path)\n",
    "\n",
    "    # Collect first 5 “correct→incorrect” cases\n",
    "    if len(mis_examples_pgd) < 5:\n",
    "        with torch.no_grad():\n",
    "            pred_o = model((img - mean)/std).argmax(1).item()\n",
    "            pred_a = model((adv - mean)/std).argmax(1).item()\n",
    "        if pred_o == true_idx and pred_a != true_idx:\n",
    "            mis_examples_pgd.append((img.cpu(), adv.cpu(), true_idx))\n",
    "\n",
    "print(f\"[PGD] Maximum L∞ distance: {max_linf_pgd:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fd485",
   "metadata": {},
   "source": [
    "### 3. Evaluate PGD Attack Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc25396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build labels list for 500 adversarial images\n",
    "expanded_labels = [numeric_labels[i] for i in range(100) for _ in range(5)]\n",
    "\n",
    "# Dataset & DataLoader for PGD images\n",
    "adv_ds_pgd = AdvDataset(pgd_dir, expanded_labels, NORM_TRANSFORM)\n",
    "adv_loader = DataLoader(adv_ds_pgd, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "sum1 = sum5 = n = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in adv_loader:\n",
    "        x, y = x.to(device), torch.tensor(y, device=device)\n",
    "        logits = model(x)  # full-image PGD already applied\n",
    "        acc = compute_topk_accuracy(logits, y, topk=(1,5))\n",
    "        bsz = y.size(0)\n",
    "        sum1 += acc[1] * bsz / 100.0\n",
    "        sum5 += acc[5] * bsz / 100.0\n",
    "        n    += bsz\n",
    "\n",
    "pgd_top1 = 100.0 * sum1 / n\n",
    "pgd_top5 = 100.0 * sum5 / n\n",
    "\n",
    "# Compute drops relative to baseline\n",
    "drop1 = (baseline_top1 - pgd_top1) / baseline_top1 * 100\n",
    "drop5 = (baseline_top5 - pgd_top5) / baseline_top5 * 100\n",
    "\n",
    "print(f\"PGD Top-1: {pgd_top1:.2f}%  (Drop {drop1:.2f}%)\")\n",
    "print(f\"PGD Top-5: {pgd_top5:.2f}%  (Drop {drop5:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034aaa9d",
   "metadata": {},
   "source": [
    "### 4. Visualize a Few “Correct→Incorrect” Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attack(img_orig, img_adv, label):\n",
    "    pert = img_adv - img_orig\n",
    "    pert = (pert - pert.min()) / (pert.max() - pert.min() + 1e-8)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(to_pil_image(img_orig.squeeze()))\n",
    "    axs[0].set_title(\"Original\");    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(to_pil_image(img_adv.squeeze()))\n",
    "    axs[1].set_title(\"Adversarial\"); axs[1].axis(\"off\")\n",
    "    axs[2].imshow(to_pil_image(pert.squeeze()))\n",
    "    axs[2].set_title(\"Noise\");       axs[2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for img_o, img_a, lbl in mis_examples_pgd:\n",
    "    visualize_attack(img_o, img_a, lbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d4255",
   "metadata": {},
   "source": [
    "## Task 4: Patch-based PGD Attack\n",
    "\n",
    "In this task we restrict the PGD attack to a small patch, generate adversarial examples, evaluate performance drop, and visualize a few cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff3e7b",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f110b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and hyperparameters\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epsilon    = 0.4\n",
    "alpha      = 0.025\n",
    "iters      = 30\n",
    "patch_size = 32\n",
    "\n",
    "patch_dir = \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/Adversarial_Test_Set_3\"\n",
    "os.makedirs(patch_dir, exist_ok=True)\n",
    "\n",
    "# Load model (reuse ResNet-34 from Task 1)\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Normalization tensors\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c721b",
   "metadata": {},
   "source": [
    "### 2. Generate Adversarial Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd670c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for raw images\n",
    "raw_ds = datasets.ImageFolder(dataset_pa, transform=RAW_TRANSFORM)\n",
    "loader = DataLoader(raw_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "mis_examples_patch = []\n",
    "max_linf_patch     = 0.0\n",
    "\n",
    "for idx, (img, folder_idx) in enumerate(tqdm(loader)):\n",
    "    img    = img.to(device)\n",
    "    true_idx = numeric_labels[folder_idx.item()]\n",
    "    y      = torch.tensor([true_idx], device=device)\n",
    "\n",
    "    # Run patch-constrained PGD\n",
    "    adv = pgd_patch_attack(model, img, y,\n",
    "                           epsilon, alpha, iters,\n",
    "                           mean, std, patch_size,\n",
    "                           random_patch=False)\n",
    "\n",
    "    # Track max perturbation\n",
    "    linf = (adv - img).abs().max().item()\n",
    "    max_linf_patch = max(max_linf_patch, linf)\n",
    "\n",
    "    # Save adversarial image\n",
    "    save_path = os.path.join(patch_dir, f\"{idx:04d}.png\")\n",
    "    save_image(adv.cpu(), save_path)\n",
    "\n",
    "    # Collect first 5 “correct→incorrect” cases\n",
    "    if len(mis_examples_patch) < 5:\n",
    "        with torch.no_grad():\n",
    "            pred_o = model((img - mean)/std).argmax(1).item()\n",
    "            pred_a = model((adv - mean)/std).argmax(1).item()\n",
    "        if pred_o == true_idx and pred_a != true_idx:\n",
    "            mis_examples_patch.append((img.cpu(), adv.cpu(), true_idx))\n",
    "\n",
    "print(f\"[Patch] Max L∞ perturbation: {max_linf_patch:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af423a",
   "metadata": {},
   "source": [
    "### 3. Evaluate Patch Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d473442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build labels list for 500 adversarial patch images\n",
    "expanded_labels = [numeric_labels[i] for i in range(100) for _ in range(5)]\n",
    "\n",
    "# Dataset & DataLoader\n",
    "adv_ds_patch = AdvDataset(patch_dir, expanded_labels, NORM_TRANSFORM)\n",
    "adv_loader   = DataLoader(adv_ds_patch, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "sum1 = sum5 = n = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in adv_loader:\n",
    "        x, y = x.to(device), torch.tensor(y, device=device)\n",
    "        logits = model(x)  # no extra normalization needed here\n",
    "        acc = compute_topk_accuracy(logits, y, topk=(1,5))\n",
    "        bsz = y.size(0)\n",
    "        sum1 += acc[1] * bsz / 100.0\n",
    "        sum5 += acc[5] * bsz / 100.0\n",
    "        n    += bsz\n",
    "\n",
    "patch_top1 = 100.0 * sum1 / n\n",
    "patch_top5 = 100.0 * sum5 / n\n",
    "\n",
    "print(f\"[Patch] Top-1 Accuracy: {patch_top1:.2f}%\")\n",
    "print(f\"[Patch] Top-5 Accuracy: {patch_top5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9013d15",
   "metadata": {},
   "source": [
    "### 4. Visualize Patch Attack Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4666bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attack(img_orig, img_adv, label):\n",
    "    pert = img_adv - img_orig\n",
    "    pert = (pert - pert.min()) / (pert.max() - pert.min() + 1e-8)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(to_pil_image(img_orig.squeeze()))\n",
    "    axs[0].set_title(\"Original\");    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(to_pil_image(img_adv.squeeze()))\n",
    "    axs[1].set_title(\"Adversarial\"); axs[1].axis(\"off\")\n",
    "    axs[2].imshow(to_pil_image(pert.squeeze()))\n",
    "    axs[2].set_title(\"Noise\");       axs[2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for img_o, img_a, lbl in mis_examples_patch:\n",
    "    visualize_attack(img_o, img_a, lbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0662f",
   "metadata": {},
   "source": [
    "## Task 5: Transferability of Adversarial Examples\n",
    "\n",
    "In this task we evaluate how the adversarial sets generated in Tasks 2–4 transfer to a different model (DenseNet-121). We compute top-1 and top-5 accuracy on the original and three adversarial datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3a07b",
   "metadata": {},
   "source": [
    "### 1. Setup Device, Model, and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4974f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained DenseNet-121 and set to eval mode\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# L∞ normalization tensors\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07195c23",
   "metadata": {},
   "source": [
    "### 2. Prepare Labels and Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d50953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global labels list (500 entries: 100 classes × 5 each)\n",
    "expanded_labels = [numeric_labels[i] for i in range(100) for _ in range(5)]\n",
    "\n",
    "# Paths to original and adversarial datasets\n",
    "datasets_paths = {\n",
    "    \"Original\":          dataset_path,\n",
    "    \"AdvTestSet1_FGSM\":  \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/Adversarial_Test_Set_1\",\n",
    "    \"AdvTestSet2_PGD\":   \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/Adversarial_Test_Set_2\",\n",
    "    \"AdvTestSet3_Patch\": \"/content/drive/MyDrive/NYU CS/Deep Learning/project3/Adversarial_Test_Set_3\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a858158",
   "metadata": {},
   "source": [
    "### 3. Evaluate Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4260e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for name, path in datasets_paths.items():\n",
    "    model.eval()\n",
    "    sum1 = sum5 = n = 0\n",
    "\n",
    "    # Choose transform: RAW for original, normalized for adversarial\n",
    "    if name == \"Original\":\n",
    "        ds = datasets.ImageFolder(path, transform=RAW_TRANSFORM)\n",
    "    else:\n",
    "        ds = AdvDataset(path, expanded_labels, NORM_TRANSFORM)\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            # Build target tensor\n",
    "            if name == \"Original\":\n",
    "                y = torch.tensor([numeric_labels[idx] for idx in y.tolist()], device=device)\n",
    "            else:\n",
    "                y = torch.tensor(y, device=device)\n",
    "\n",
    "            # Forward pass with normalization\n",
    "            logits = model((x - mean) / std)\n",
    "\n",
    "            # Compute top-1 and top-5\n",
    "            acc = compute_topk_accuracy(logits, y, topk=(1,5))\n",
    "            bsz = y.size(0)\n",
    "            sum1 += acc[1] * bsz / 100.0\n",
    "            sum5 += acc[5] * bsz / 100.0\n",
    "            n    += bsz\n",
    "\n",
    "    # Store results\n",
    "    results.append((name, n, 100.0*sum1/n, 100.0*sum5/n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995936c9",
   "metadata": {},
   "source": [
    "### 4. Print Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 5 Results: DenseNet-121 Performance ---\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(f\"{'Dataset':<25} | {'Samples':<7} | {'Top-1 Acc (%)':<14} | {'Top-5 Acc (%)'}\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "for name, total, acc1, acc5 in results:\n",
    "    print(f\"{name:<25} | {total:<7} | {acc1:<14.2f} | {acc5:.2f}\")\n",
    "print(\"-------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
